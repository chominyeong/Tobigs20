{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fu39oBW0RVn5"
   },
   "source": [
    "# [과제 3] 로지스틱 회귀분석\n",
    "### - sklearn 패키지를 사용해 로지스틱 회귀분석을 진행해주세요.\n",
    "### - 성능지표를 계산하고 이에 대해 해석해주세요.\n",
    "### - 성능 개선을 시도해주세요. (어떠한 성능지표를 기준으로 개선을 시도했는지, 그 이유도 함께 적어주세요.)\n",
    "### - 주석으로 설명 및 근거 자세하게 달아주시면 감사하겠습니다. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rN2SWezRVn_"
   },
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7SYKNvQRVn_"
   },
   "source": [
    "출처 : https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "\n",
    "* V1 ~ V28 : 비식별화 된 개인정보 \n",
    "* **Class** : Target 변수  \n",
    "  - 1 : fraudulent transactions (사기)\n",
    "  - 0 : otherwise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Uvjw2fTCRVoA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "znQit70ZRVoA"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"assignment3_creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 220
    },
    "id": "v98OeXW5RVoB",
    "outputId": "42afeddc-07e6-4224-95ee-08b455f72475"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.848212</td>\n",
       "      <td>2.384900</td>\n",
       "      <td>0.379573</td>\n",
       "      <td>1.048381</td>\n",
       "      <td>-0.845070</td>\n",
       "      <td>2.537837</td>\n",
       "      <td>-4.542983</td>\n",
       "      <td>-10.201458</td>\n",
       "      <td>-1.504967</td>\n",
       "      <td>-2.234167</td>\n",
       "      <td>...</td>\n",
       "      <td>2.585817</td>\n",
       "      <td>-5.291690</td>\n",
       "      <td>0.859364</td>\n",
       "      <td>0.423231</td>\n",
       "      <td>-0.506985</td>\n",
       "      <td>1.020052</td>\n",
       "      <td>-0.627751</td>\n",
       "      <td>-0.017753</td>\n",
       "      <td>0.280982</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.071805</td>\n",
       "      <td>-0.477943</td>\n",
       "      <td>-1.444444</td>\n",
       "      <td>-0.548657</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>-0.582242</td>\n",
       "      <td>-0.042878</td>\n",
       "      <td>-0.247160</td>\n",
       "      <td>1.171923</td>\n",
       "      <td>-0.342382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077306</td>\n",
       "      <td>0.042858</td>\n",
       "      <td>0.390125</td>\n",
       "      <td>0.041569</td>\n",
       "      <td>0.598427</td>\n",
       "      <td>0.098803</td>\n",
       "      <td>0.979686</td>\n",
       "      <td>-0.093244</td>\n",
       "      <td>-0.065615</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.985294</td>\n",
       "      <td>-2.747472</td>\n",
       "      <td>1.194068</td>\n",
       "      <td>-0.003036</td>\n",
       "      <td>-1.151041</td>\n",
       "      <td>-0.263559</td>\n",
       "      <td>0.553500</td>\n",
       "      <td>0.635600</td>\n",
       "      <td>0.438545</td>\n",
       "      <td>-1.806488</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345776</td>\n",
       "      <td>0.373760</td>\n",
       "      <td>-0.385777</td>\n",
       "      <td>1.197596</td>\n",
       "      <td>0.407229</td>\n",
       "      <td>0.008013</td>\n",
       "      <td>0.762362</td>\n",
       "      <td>-0.299024</td>\n",
       "      <td>-0.303929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.479452</td>\n",
       "      <td>1.542874</td>\n",
       "      <td>0.290895</td>\n",
       "      <td>0.838142</td>\n",
       "      <td>-0.529290</td>\n",
       "      <td>-0.717661</td>\n",
       "      <td>0.484516</td>\n",
       "      <td>0.545092</td>\n",
       "      <td>-0.780767</td>\n",
       "      <td>0.324804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038397</td>\n",
       "      <td>0.116771</td>\n",
       "      <td>0.405560</td>\n",
       "      <td>-0.116453</td>\n",
       "      <td>0.541275</td>\n",
       "      <td>-0.216665</td>\n",
       "      <td>-0.415578</td>\n",
       "      <td>0.027126</td>\n",
       "      <td>-0.150347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.281976</td>\n",
       "      <td>-0.309699</td>\n",
       "      <td>-2.162299</td>\n",
       "      <td>-0.851514</td>\n",
       "      <td>0.106167</td>\n",
       "      <td>-1.483888</td>\n",
       "      <td>1.930994</td>\n",
       "      <td>-0.843049</td>\n",
       "      <td>-1.249272</td>\n",
       "      <td>1.079608</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.875516</td>\n",
       "      <td>-0.004199</td>\n",
       "      <td>1.015108</td>\n",
       "      <td>-0.026748</td>\n",
       "      <td>0.077115</td>\n",
       "      <td>-1.468822</td>\n",
       "      <td>0.751700</td>\n",
       "      <td>0.496732</td>\n",
       "      <td>0.331001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.848212  2.384900  0.379573  1.048381 -0.845070  2.537837 -4.542983   \n",
       "1  2.071805 -0.477943 -1.444444 -0.548657  0.010036 -0.582242 -0.042878   \n",
       "2 -2.985294 -2.747472  1.194068 -0.003036 -1.151041 -0.263559  0.553500   \n",
       "3 -1.479452  1.542874  0.290895  0.838142 -0.529290 -0.717661  0.484516   \n",
       "4 -0.281976 -0.309699 -2.162299 -0.851514  0.106167 -1.483888  1.930994   \n",
       "\n",
       "          V8        V9       V10  ...       V20       V21       V22       V23  \\\n",
       "0 -10.201458 -1.504967 -2.234167  ...  2.585817 -5.291690  0.859364  0.423231   \n",
       "1  -0.247160  1.171923 -0.342382  ... -0.077306  0.042858  0.390125  0.041569   \n",
       "2   0.635600  0.438545 -1.806488  ...  1.345776  0.373760 -0.385777  1.197596   \n",
       "3   0.545092 -0.780767  0.324804  ...  0.038397  0.116771  0.405560 -0.116453   \n",
       "4  -0.843049 -1.249272  1.079608  ... -0.875516 -0.004199  1.015108 -0.026748   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  \n",
       "0 -0.506985  1.020052 -0.627751 -0.017753  0.280982      0  \n",
       "1  0.598427  0.098803  0.979686 -0.093244 -0.065615      0  \n",
       "2  0.407229  0.008013  0.762362 -0.299024 -0.303929      0  \n",
       "3  0.541275 -0.216665 -0.415578  0.027126 -0.150347      0  \n",
       "4  0.077115 -1.468822  0.751700  0.496732  0.331001      0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"Class\"],axis=1)\n",
    "y = data[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로지스틱 회귀\n",
    "회귀를 사용하여 데이터가 어떤 범주에 속할 확률을 0과 1 사이의 값으로 예측하고 그 확률에 따라 가능성이 더 높은 범주에 속하는 것으로 분류하는 알고리즘\n",
    "\n",
    "$p(X) = \\frac{1}{1+e^{-f(x)}} = \\frac{1}{1+e^{- (w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n)}} = \\frac{1}{1+e^{-Z}}  $  \n",
    "$(Z=XW)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameters\n",
    "\n",
    "**penalty**\n",
    "- 일반적인 logistic : penalty=None\n",
    "- 릿지 logistic : penalty=\"l2\"\n",
    "- 라쏘 logistic : penalty=\"l1\"\n",
    "- 병합 logistic : penalty=\"elasticnet\"\n",
    "\n",
    "**solver**\n",
    "- ‘lbfgs’ - [‘l2’, None]\n",
    "- ‘liblinear’ - [‘l1’, ‘l2’]\n",
    "- ‘newton-cg’ - [‘l2’, None]\n",
    "- ‘newton-cholesky’ - [‘l2’, None]\n",
    "- ‘sag’ - [‘l2’, None]\n",
    "- ‘saga’ - [‘elasticnet’, ‘l1’, ‘l2’, None]\n",
    "\n",
    "**C**   \n",
    "alpha의 역수(높은 C ; 낮은 강도의 제약조건 / 낮은 C ; 높은 강도의 제약조건)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logitS = LogisticRegression(penalty=None)\n",
    "logitS.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logitS.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성능지표"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|CM|예측 negative|예측 positive|\n",
    "|:---:|:---:|:---:|\n",
    "|실제 negative|TN|FP|\n",
    "|실제 positive|FN|TP|\n",
    "\n",
    "**정확도 accuracy**  전체 데이터 중 실제 맞힌 데이터 수    \n",
    "= (TP + TN) / (TP + TN + EP + FN)\n",
    "\n",
    "**정밀도 precision** positive로 예측한 것 중 실제 positive의 비율   \n",
    "= TP / (TP + FP)\n",
    "\n",
    "**재현율 recall** 실제 positive 중 positive로 예측한 것의 비율   \n",
    "= TP / (TP + FN)\n",
    "\n",
    "**F1-score** 재현율과 정밀도가 동시에 높아야 예측력이 좋은 분류 모델이므로 둘의 조화평균을 구한 값   \n",
    "= (2*정밀도*재현율) / (정밀도 + 재현율)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8535    2]\n",
      " [  10   57]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8535, 2, 10, 57)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred).ravel()\n",
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.99860529986053\n",
      "정밀도: 0.9661016949152542\n",
      "재현율: 0.8507462686567164\n",
      "f1 점수: 0.9047619047619047\n"
     ]
    }
   ],
   "source": [
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"정확도:\", accuracy)\n",
    "\n",
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "print(\"정밀도:\", precision)\n",
    "\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "print(\"재현율:\", recall)\n",
    "\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "print(\"f1 점수:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 4가지 성능 지표에서는 성능이 좋은 것으로 나타남.    \n",
    "주어진 데이터는 클래스 분포가 불균형하기 때문에 성능을 높이는 데 사용하는 지표로 F1 score를 사용하겠음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성능 높이기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Class', ylabel='count'>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=data['Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    28432\n",
       "1      246\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class의 불균형이 매우 크므로 oversampling 또는 undersampling 필요    \n",
    "class == 0 인 데이터를 undersampling 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 무작위undersampling\n",
    "from imblearn.under_sampling import *\n",
    "X_samp, y_samp = RandomUnderSampler(random_state=0).fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    246\n",
       "1    246\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_samp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_samp, y_samp, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logitS = LogisticRegression(penalty=None)\n",
    "logitS.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logitS.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 점수: 0.9178082191780821\n"
     ]
    }
   ],
   "source": [
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "print(\"f1 점수:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "# SMOTE 인스턴스 생성\n",
    "oversampling_instance = SMOTE(k_neighbors=3)\n",
    "\n",
    "# 오버샘플링 적용\n",
    "X_samp, y_samp =oversampling_instance.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    28432\n",
       "1    28432\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_samp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_samp, y_samp, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 점수: 0.9566771944460998\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logitS = LogisticRegression(penalty=None)\n",
    "logitS.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logitS.predict(X_test)\n",
    "\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "print(\"f1 점수:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "undersampling 보다 smote oversampling을 했을 때 f1-score가 높아진 것을 알 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. penalty 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적인 로지스틱 회귀(penalty=None) 일때와 릿지, 라쏘, 엘라스틱넷으로 변경했을 때의 성능지표 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 점수: 0.9565476545270312\n"
     ]
    }
   ],
   "source": [
    "# 릿지\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logitS = LogisticRegression(penalty=\"l2\")\n",
    "logitS.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logitS.predict(X_test)\n",
    "\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "print(\"f1 점수:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[0;32m      4\u001b[0m logitS \u001b[38;5;241m=\u001b[39m LogisticRegression(penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mlogitS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m logitS\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      9\u001b[0m f1 \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mf1_score(y_test, y_pred)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1162\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;124;03mFit the model according to the given training data.\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;124;03mThe SAGA solver supports both float64 and float32 bit arrays.\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m-> 1162\u001b[0m solver \u001b[38;5;241m=\u001b[39m \u001b[43m_check_solver\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melasticnet\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml1_ratio \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1_ratio parameter is only used when penalty is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124melasticnet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(penalty=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpenalty)\n\u001b[0;32m   1169\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:54\u001b[0m, in \u001b[0;36m_check_solver\u001b[1;34m(solver, penalty, dual)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_solver\u001b[39m(solver, penalty, dual):\n\u001b[0;32m     51\u001b[0m \n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# TODO(1.4): Remove \"none\" option\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m penalty \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 54\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSolver \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m supports only \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m penalties, got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m penalty.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m             \u001b[38;5;241m%\u001b[39m (solver, penalty)\n\u001b[0;32m     57\u001b[0m         )\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dual:\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     60\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSolver \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m supports only dual=False, got dual=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (solver, dual)\n\u001b[0;32m     61\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty."
     ]
    }
   ],
   "source": [
    "# 라쏘\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logitS = LogisticRegression(penalty=\"l1\")\n",
    "logitS.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logitS.predict(X_test)\n",
    "\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "print(\"f1 점수:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오류 원인) parameter인 Solver는 default가 ‘lbfgs’인데, 이는 'l2'와 'none'만 지원.   \n",
    "해결을 위해 Solver 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 점수: 0.9561686372865161\n"
     ]
    }
   ],
   "source": [
    "# 라쏘\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logitS = LogisticRegression(solver='saga',penalty=\"l1\")\n",
    "logitS.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logitS.predict(X_test)\n",
    "\n",
    "f1 = metrics.f1_score(y_test, y_pred)\n",
    "print(\"f1 점수:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "회귀에 penalty를 주는 것에는 성능에 큰 차이가 없음.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최종모형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smote oversampling을 한 일반적인 로지스틱 회귀모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Regression_과제3",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
